{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 30, 160, 320, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liyi/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# 读取一个 h5 图片数据集，便于后面使用\n",
    "# liyi，2019/5/19\n",
    "\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "path = '/home/liyi/video-pred/pytorch_video_pred/data/comma/train/'\n",
    "# print(sorted(os.listdir(path))[0])\n",
    "batch_size = 6\n",
    "idx = range(batch_size)\n",
    "files = ['%010d.h5'% i for i in idx]\n",
    "inputs = []\n",
    "for file in files:\n",
    "    f = h5py.File(path+file, 'r')\n",
    "    sample = dict(f)['image'].value.astype(np.float32)\n",
    "    inputs.append(sample)\n",
    "    f.close()\n",
    "    \n",
    "inputs = np.array(inputs)\n",
    "print(inputs.shape)  # NDHWC 5/22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HParams([('context_frames', -1), ('repeat', 1), ('sequence_length', -1)])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取超参数，方便后面调试\n",
    "# from base_model.py\n",
    "# liyi，2019/5/19\n",
    "\n",
    "from tensorflow.contrib.training import HParams\n",
    "\n",
    "def get_hparams(hparams_dict=None):\n",
    "    hparams = dict(\n",
    "        context_frames=-1,\n",
    "        sequence_length=-1,\n",
    "        repeat=1,\n",
    "    )\n",
    "    hparams.update(hparams_dict or {})\n",
    "    return HParams(**hparams)\n",
    "\n",
    "get_hparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为下面测试 posterior 准备超参数 5/22\n",
    "hparams = dict(\n",
    "    dataset='bair',\n",
    "    input_dir='/home/liyi/video-pred/video_prediction/data/comma_m/train',\n",
    "    model='savp',\n",
    "    model_hparams_dict='hparams/bair_action_free/ours_savp/model_hparams.json',\n",
    "    \n",
    "    l1_weight=1.0,\n",
    "    l2_weight=0.0,\n",
    "    n_layers=3,  # 3改为5 5/21\n",
    "    ndf=32,\n",
    "    norm_layer='instance',\n",
    "    use_same_discriminator=False,\n",
    "    ngf=32,\n",
    "    downsample_layer='conv_pool2d',\n",
    "    upsample_layer='upsample_conv2d',\n",
    "    activation_layer='relu',  # for generator only\n",
    "    transformation='cdna',\n",
    "    kernel_size=(5, 5),\n",
    "    dilation_rate=(1, 1),\n",
    "    where_add='all',\n",
    "    use_tile_concat=True,\n",
    "    learn_initial_state=False,\n",
    "    rnn='lstm',\n",
    "    conv_rnn='lstm',\n",
    "    conv_rnn_norm_layer='instance',\n",
    "    num_transformed_images=4,\n",
    "    last_frames=1,\n",
    "    prev_image_background=True,\n",
    "    first_image_background=True,\n",
    "    last_image_background=False,\n",
    "    last_context_image_background=False,\n",
    "    context_images_background=False,\n",
    "    generate_scratch_image=True,\n",
    "    dependent_mask=True,\n",
    "    schedule_sampling='inverse_sigmoid',\n",
    "    schedule_sampling_k=900.0,\n",
    "    schedule_sampling_steps=(0, 100000),\n",
    "    use_e_rnn=False,\n",
    "    learn_prior=False,\n",
    "    nz=8,\n",
    "    num_samples=8,\n",
    "    nef=64,   ### 64改为32 5/21\n",
    "    use_rnn_z=True,\n",
    "    ablation_conv_rnn_norm=False,\n",
    "    ablation_rnn=False,\n",
    ")\n",
    "\n",
    "parsed_hparams = get_hparams(hparams_dict=hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试 prior\n",
    "# from module.py\n",
    "# liyi,2019/5/22\n",
    "\n",
    "import functools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from video_prediction.utils.max_sv import spectral_normed_weight\n",
    "from video_prediction.layers.conv import Conv2d, Conv3d\n",
    "from video_prediction.models.modules import Encoder,Dense\n",
    "\n",
    "class Prior(nn.Module):\n",
    "    ### 改写自savp_model.py prior_fn 5/16\n",
    "    def __init__(self, input_shape, hparams):\n",
    "        super(Prior, self).__init__()\n",
    "        self.hparams = hparams\n",
    "        self.encoder = Encoder(input_shape, nef=hparams.nef, n_layers=hparams.n_layers)### input_shape需要根据hparmas修改 5/16\n",
    "        self.dense0 = Dense(input_shape, units=hparams.nef * 4)\n",
    "        if hparams.rnn == 'lstm':\n",
    "            self.rnn = nn.LSTM(hidden_size=hparams.nef * 4)\n",
    "        elif hparams.rnn == 'gru':\n",
    "            self.rnn = nn.GRU(hidden_size=hparams.nef * 4)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        self.dense1 = Dense(input_shape=hparams.nef*4, units=hparams.nz)  ### input_shape要改 5/16\n",
    "        self.dense2 = Dense(input_shape=hparams.nef*4, units=hparams.nz)  ### input_shape要改 5/16\n",
    "        \n",
    "    def forward(inputs):\n",
    "        ### inputs应当是 NCHW 5/16\n",
    "        outputs = {}\n",
    "        ### 将连续的两帧图片在channel维度上级联 5/16\n",
    "        ### context_frams 需要根据 ... 5/16\n",
    "        inputs = torch.cat([inputs[:self.hparams.context_frames - 1], inputs[1:self.hparams.context_frames]], dim=-3)  \n",
    "        ### 加入 action uncompleted ... \n",
    "        h = self.encoder(inputs)\n",
    "        h_zeros = torch.zeros(sizes = torch.cat(\n",
    "            [[self.hparams.sequence_length - self.hparams.context_frames], h.size[1:]], axis=0))\n",
    "        \n",
    "        h = torch.cat([h, h_zeros], axis=0)\n",
    "        h = self.dense0(h)\n",
    "        h = self.rnn(h)\n",
    "        z_mu = self.dense1(h)\n",
    "        outputs['z_mu'] = z_mu\n",
    "        z_log_sigma_sq = self.dense2(h)\n",
    "        z_log_sigma_sq = torch.clamp(z_log_sigma_sq, -10,10)\n",
    "        outputs['z_log_sigma_sq'] = z_log_sigma_sq\n",
    "        return outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[180, 256]\n",
      "torch.Size([174, 6, 160, 320])\n",
      "z_mu torch.Size([29, 6, 8])\n",
      "z_log_sigma_sq torch.Size([29, 6, 8])\n"
     ]
    }
   ],
   "source": [
    "# 测试 posterior\n",
    "# from modules.py\n",
    "# liyi,2019/5/22\n",
    "# input 为 DNCHW 5/22\n",
    "# output 为 D-1,N,nz[=8] 5/22\n",
    "\n",
    "import functools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from video_prediction.utils.max_sv import spectral_normed_weight\n",
    "from video_prediction.layers.conv import Conv2d, Conv3d\n",
    "from video_prediction.models.modules import Encoder,Dense\n",
    "\n",
    "class Posterior(nn.Module):\n",
    "    ### 改写自savp_model.py posterior_fn 5/15\n",
    "    ### input 为 DNCHW 5/19\n",
    "    ### output 为 D-1,N,nz[=8] 5/21\n",
    "    def __init__(self, input_shape, hparams):\n",
    "        super(Posterior, self).__init__()\n",
    "        self.input_shape = list(input_shape)\n",
    "        self.use_e_rnn = hparams.use_e_rnn  ### 默认false 5/19\n",
    "        \n",
    "        #input_shape = list(input_shape)\n",
    "        #input_shape=[np.prod(input_shape[0:2])]+[input_shape[-3]*2]+input_shape[-2:]\n",
    "        #print(input_shape)\n",
    "        self.encoder = Encoder(input_shape=[np.prod(self.input_shape[0:2])]+[self.input_shape[-3]*2]+self.input_shape[-2:],\n",
    "                               nef=hparams.nef, n_layers=hparams.n_layers)\n",
    "        out_shape = [np.prod(self.input_shape[0:2]),\n",
    "                     hparams.nef * min(4, 2**(hparams.n_layers-1))]\n",
    "        print(out_shape)\n",
    "        self.dense1 = Dense(input_shape=out_shape, units=hparams.nz)  ### input_shape要改 5/15\n",
    "        self.dense2 = Dense(input_shape=out_shape, units=hparams.nz)  ### input_shape要改 5/15\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        ### inputs应当是 NDCHW 5/16\n",
    "        outputs = {}\n",
    "        inputs = torch.cat([inputs[:-1], inputs[1:]], dim=-3) ### 将连续的两帧图片在channel维度上级联 5/16\n",
    "        inputs = inputs.reshape([-1]+list(inputs.shape[-3:]))  ### 变为 NCHW 5/22\n",
    "        print(inputs.shape)\n",
    "        ### 加入 action uncompleted ... \n",
    "        h = self.encoder(inputs)['output']\n",
    "        if self.use_e_rnn:\n",
    "            h = self.dense0(h)\n",
    "            h = self.rnn(h)\n",
    "        z_mu = self.dense1(h).reshape([self.input_shape[0]-1]+[self.input_shape[1]]+[-1])\n",
    "        outputs['z_mu'] = z_mu\n",
    "        z_log_sigma_sq = self.dense2(h).reshape([self.input_shape[0]-1]+[self.input_shape[1]]+[-1])\n",
    "        z_log_sigma_sq = torch.clamp(z_log_sigma_sq, -10,10)\n",
    "        outputs['z_log_sigma_sq'] = z_log_sigma_sq\n",
    "        return outputs\n",
    "    \n",
    "images = torch.tensor(np.transpose(inputs,[1,0,4,2,3]))  # to DNCHW 5/22\n",
    "net = Posterior(input_shape=images.shape, hparams=parsed_hparams)\n",
    "output = net.forward(images)\n",
    "for k in output.keys():\n",
    "    print(k, output[k].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 30, 320, 3, 160])\n",
      "conv:  torch.Size([30, 256, 1, 20])\n",
      "pool:  torch.Size([30, 256, 1, 1])\n",
      "torch.Size([30, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liyi/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "# 测试encoder\n",
    "# from modules.py\n",
    "# liyi，2019/5/22\n",
    "\n",
    "import functools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from video_prediction.utils.max_sv import spectral_normed_weight\n",
    "from video_prediction.layers.conv import Conv2d, Conv3d\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    ### conv2d的in_channels是否为3存疑 5/8\n",
    "    ### nef 为 encoder 的 filter 个数 5/9\n",
    "    ### conv2d 要求 input_shape = NCHW\n",
    "    ### input_shape = (ND/DN,CHW) 5/22\n",
    "    ### output_shape = (ND/DN,min(2**(n_layers-1),4)),只有三个维度 5/21\n",
    "    def __init__(self, input_shape, nef=64, n_layers=3):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.conv = {}\n",
    "        self.norm = {}\n",
    "        self.conv0 = nn.Conv2d(in_channels=self.input_shape[-3], \n",
    "                               out_channels=nef, kernel_size=4, stride=2, padding=(1,1))\n",
    "        def make_sequence(in_channel, i):\n",
    "            out_channel = nef * min(2**i, 4)\n",
    "            return [nn.Conv2d(\n",
    "                        in_channels=in_channel,\n",
    "                        out_channels=out_channel, \n",
    "                        kernel_size=4, stride=2,\n",
    "                        padding=(1,1)),\n",
    "                      nn.InstanceNorm2d(\n",
    "                        num_features=out_channel,\n",
    "                        eps=1e-6)], out_channel\n",
    "        \n",
    "        self.model_list = nn.ModuleList()\n",
    "        in_channel = nef\n",
    "        for i in range(1, n_layers):\n",
    "            sequence, in_channel = make_sequence(in_channel, i)\n",
    "            self.model_list += sequence\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        ### inputs 应当是 NCHW 5/8\n",
    "        outputs = {}\n",
    "        output = self.conv0(inputs)\n",
    "        output = F.leaky_relu(output, negative_slope=0.2)\n",
    "        n = 0\n",
    "        outputs['encoder_%d'%n] = output     ### for visualization 5/8\n",
    "        for model in self.model_list:\n",
    "            n += 1\n",
    "            output = model(output)\n",
    "            output = F.leaky_relu(output, negative_slope=0.2)\n",
    "            outputs['encoder_%d'%n] = output\n",
    "        print('conv: ',output.shape)\n",
    "        output = F.avg_pool2d(output, output.shape[2:])\n",
    "        print('pool: ',output.shape)\n",
    "        output.squeeze_(dim=-2)  # 对HW两个维度squeeze\n",
    "        output.squeeze_(dim=-1)\n",
    "        outputs['output'] = output\n",
    "        return outputs\n",
    "    \n",
    "inputs = torch.tensor(np.transpose(inputs,axes=[0,1,4,2,3])) # NDHWC to NDCHW 5/22\n",
    "print(inputs.shape)\n",
    "net = Encoder(input_shape=inputs[0].shape)\n",
    "outputs = net(inputs[0])\n",
    "print(outputs['output'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n",
      "[  2  12 160 320   3]\n",
      "[ -1 160 320   3]\n",
      "(4,)\n",
      "[ -1 160 320   3]\n",
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "ndims = 4\n",
    "x = tf.zeros([2,12,160,320,3])\n",
    "\n",
    "sess = tf.Session()\n",
    "print(x1.get_shape())\n",
    "\n",
    "shape = tf.shape(x)\n",
    "print(sess.run(shape))\n",
    "\n",
    "x1 = tf.concat([[-1], shape[-(ndims-1):]], axis=0)\n",
    "print(sess.run(x1))\n",
    "print(x1.get_shape())\n",
    "\n",
    "x1.set_shape([ndims])\n",
    "print(sess.run(x1))\n",
    "print(x1.get_shape())\n",
    "\n",
    "#sess = tf.Session()\n",
    "#print(sess.run(tf.shape(x1), feed_dict={x1:[0,1,2,3]}))\n",
    "#x1.set_shape([2,2])\n",
    "#print(sess.run(tf.shape(x1)))\n",
    "#print(sess.run(tf.shape(x1), feed_dict={x1:[[0,1],[2,3]]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 4],\n",
      "        [2, 3, 4]])\n",
      "tensor([[1, 2],\n",
      "        [4, 2],\n",
      "        [3, 4]])\n",
      "torch.Size([3, 2])\n",
      "tensor([[1, 2, 4, 2, 3, 4]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([[1,2,4],[2,3,4]])\n",
    "b = a.reshape([-1,2])\n",
    "print(a)\n",
    "print(b)\n",
    "print(b.shape)\n",
    "b = a.view([1,6])\n",
    "print(b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
