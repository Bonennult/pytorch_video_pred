{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 5, 5])\n",
      "torch.Size([1, 16, 5, 5])\n",
      "torch.Size([1, 2, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# With square kernels and equal stride\n",
    "filters = torch.randn(8,4,3,3)\n",
    "inputs = torch.randn(1,4,5,5)\n",
    "out = F.conv2d(inputs, filters, padding=1)\n",
    "print(out.shape)\n",
    "filters = torch.randn(16,2,3,3)\n",
    "out = F.conv2d(inputs, filters, groups=2, padding=1)\n",
    "print(out.shape)\n",
    "out = torch.split(out,2,dim=1)\n",
    "#for o in out:\n",
    "#    print(o.shape)\n",
    "oo = sum(out)\n",
    "print(oo.shape)\n",
    "#print(out[0].shape)\n",
    "#o = torch.stack(out[0],dim=0)\n",
    "#print(o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 30, 160, 320, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liyi/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# 读取一个 h5 图片数据集，便于后面使用\n",
    "# liyi，2019/5/19\n",
    "\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "path = '/home/liyi/video-pred/pytorch_video_pred/data/comma/train/'\n",
    "# print(sorted(os.listdir(path))[0])\n",
    "batch_size = 6\n",
    "idx = range(batch_size)\n",
    "files = ['%010d.h5'% i for i in idx]\n",
    "inputs = []\n",
    "for file in files:\n",
    "    f = h5py.File(path+file, 'r')\n",
    "    sample = dict(f)['image'].value.astype(np.float32)\n",
    "    inputs.append(sample)\n",
    "    f.close()\n",
    "    \n",
    "inputs = np.array(inputs)\n",
    "print(inputs.shape)  # NDHWC 5/22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HParams([('context_frames', -1), ('repeat', 1), ('sequence_length', -1)])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取超参数，方便后面调试\n",
    "# from base_model.py\n",
    "# liyi，2019/5/19\n",
    "\n",
    "from tensorflow.contrib.training import HParams\n",
    "\n",
    "def get_hparams(hparams_dict=None):\n",
    "    hparams = dict(\n",
    "        context_frames=-1,\n",
    "        sequence_length=-1,\n",
    "        repeat=1,\n",
    "    )\n",
    "    hparams.update(hparams_dict or {})\n",
    "    return HParams(**hparams)\n",
    "\n",
    "get_hparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liyi/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 6, 160, 320, 3)\n"
     ]
    }
   ],
   "source": [
    "# 读取一个 h5 图片数据集，便于后面使用\n",
    "# liyi，2019/5/19\n",
    "\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "path = '/home/liyi/video-pred/pytorch_video_pred/data/comma/train/'\n",
    "# print(sorted(os.listdir(path))[0])\n",
    "batch_size = 2\n",
    "idx = range(batch_size)\n",
    "files = ['%010d.h5'% i for i in idx]\n",
    "inputs = []\n",
    "for file in files:\n",
    "    f = h5py.File(path+file, 'r')\n",
    "    sample = dict(f)['image'].value.astype(np.float32)\n",
    "    inputs.append(sample)\n",
    "    f.close()\n",
    "    \n",
    "inputs = np.array(inputs)[:,0:6]   ### sequence_length = 10 6/3\n",
    "print(inputs.shape)  # NDHWC 5/22\n",
    "\n",
    "\n",
    "# 获取超参数，方便后面调试\n",
    "# from base_model.py\n",
    "# liyi，2019/5/19\n",
    "\n",
    "from tensorflow.contrib.training import HParams\n",
    "\n",
    "def get_hparams(hparams_dict=None):\n",
    "    hparams = dict(\n",
    "        context_frames=-1,\n",
    "        sequence_length=-1,\n",
    "        repeat=1,\n",
    "    )\n",
    "    hparams.update(hparams_dict or {})\n",
    "    return HParams(**hparams)\n",
    "\n",
    "\n",
    "# 为下面测试 posterior 准备超参数 5/22\n",
    "hparams = dict(\n",
    "    dataset='bair',\n",
    "    input_dir='/home/liyi/video-pred/video_prediction/data/comma_m/train',\n",
    "    model='savp',\n",
    "    model_hparams_dict='hparams/bair_action_free/ours_savp/model_hparams.json',\n",
    "    \n",
    "    l1_weight=1.0,\n",
    "    l2_weight=0.0,\n",
    "    n_layers=3,  # 3改为5 5/21\n",
    "    ndf=32,\n",
    "    norm_layer='instance',\n",
    "    use_same_discriminator=False,\n",
    "    ngf=32,\n",
    "    downsample_layer='conv_pool2d',\n",
    "    upsample_layer='upsample_conv2d',\n",
    "    activation_layer='relu',  # for generator only\n",
    "    transformation='cdna',\n",
    "    kernel_size=(5, 5),\n",
    "    dilation_rate=(1, 1),\n",
    "    where_add='all',\n",
    "    use_tile_concat=True,\n",
    "    learn_initial_state=False,\n",
    "    rnn='lstm',\n",
    "    conv_rnn='lstm',\n",
    "    conv_rnn_norm_layer='instance',\n",
    "    num_transformed_images=4,\n",
    "    last_frames=1,\n",
    "    prev_image_background=True,\n",
    "    first_image_background=True,\n",
    "    last_image_background=False,\n",
    "    last_context_image_background=False,\n",
    "    context_images_background=False,\n",
    "    generate_scratch_image=True,\n",
    "    dependent_mask=True,\n",
    "    schedule_sampling='inverse_sigmoid',\n",
    "    schedule_sampling_k=900.0,\n",
    "    schedule_sampling_steps=(0, 100000),\n",
    "    use_e_rnn=False,\n",
    "    learn_prior=False,\n",
    "    nz=8,\n",
    "    num_samples=8,\n",
    "    nef=64,   ### 64改为32 5/21\n",
    "    use_rnn_z=True,\n",
    "    ablation_conv_rnn_norm=False,\n",
    "    ablation_rnn=False,\n",
    "    \n",
    "    ### from base_model.py VideoPredictionModel 6/1\n",
    "    batch_size=2,\n",
    "    lr=0.001,\n",
    "    end_lr=0.0,\n",
    "    decay_steps=(200000, 300000),\n",
    "    lr_boundaries=(0,),\n",
    "    max_steps=300000,\n",
    "    beta1=0.9,\n",
    "    beta2=0.999,\n",
    "    context_frames=2,     ### -1改为2\n",
    "    sequence_length=6,   ### -1改为\n",
    "    clip_length=10,\n",
    "    vgg_cdist_weight=0.0,\n",
    "    feature_l2_weight=0.0,\n",
    "    ae_l2_weight=0.0,\n",
    "    state_weight=0.0,\n",
    "    tv_weight=0.0,\n",
    "    image_sn_gan_weight=0.0,\n",
    "    image_sn_vae_gan_weight=0.0,\n",
    "    images_sn_gan_weight=0.0,\n",
    "    images_sn_vae_gan_weight=0.0,\n",
    "    video_sn_gan_weight=0.0,\n",
    "    video_sn_vae_gan_weight=0.0,\n",
    "    gan_feature_l2_weight=0.0,\n",
    "    gan_feature_cdist_weight=0.0,\n",
    "    vae_gan_feature_l2_weight=0.0,\n",
    "    vae_gan_feature_cdist_weight=0.0,\n",
    "    gan_loss_type='LSGAN',\n",
    "    joint_gan_optimization=False,\n",
    "    kl_weight=0.0,\n",
    "    kl_anneal='linear',\n",
    "    kl_anneal_k=-1.0,\n",
    "    kl_anneal_steps=(50000, 100000),\n",
    "    z_l1_weight=0.0,\n",
    ")\n",
    "\n",
    "parsed_hparams = get_hparams(hparams_dict=hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator : inputs_posterior\n",
      "images torch.Size([6, 2, 3, 160, 320])\n",
      "zs torch.Size([5, 2, 8])\n",
      "--------------------\n",
      "Generator : inputs_prior\n",
      "images torch.Size([6, 2, 3, 160, 320])\n",
      "zs torch.Size([5, 2, 8])\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liyi/anaconda3/lib/python3.6/site-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n",
      "/home/liyi/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:2423: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n",
      "/home/liyi/video-pred/pytorch_video_pred/video_prediction/models/savp_model.py:436: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  masks = layer(masks)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeneratorGivenZ outputs\n",
      "gen_images torch.Size([5, 2, 3, 160, 320])\n",
      "transformed_images torch.Size([5, 2, 7, 3, 160, 320])\n",
      "masks torch.Size([5, 2, 7, 1, 160, 320])\n",
      "--------------------\n",
      "--------------------  gen posterior complete  --------------------\n",
      "GeneratorGivenZ outputs\n",
      "gen_images torch.Size([5, 2, 3, 160, 320])\n",
      "transformed_images torch.Size([5, 2, 7, 3, 160, 320])\n",
      "masks torch.Size([5, 2, 7, 1, 160, 320])\n",
      "--------------------\n",
      "--------------------  gen prior complete  --------------------\n",
      "Generator : inputs_samples\n",
      "images torch.Size([6, 8, 2, 3, 160, 320])\n",
      "zs torch.Size([5, 8, 2, 8])\n",
      "--------------------\n",
      "GeneratorGivenZ outputs\n",
      "gen_images torch.Size([5, 16, 3, 160, 320])\n",
      "transformed_images torch.Size([5, 16, 7, 3, 160, 320])\n",
      "masks torch.Size([5, 16, 7, 1, 160, 320])\n",
      "--------------------\n",
      "Generator samples\n",
      "Generator : gen_images_Samples   torch.Size([5, 16, 3, 160, 320])\n",
      "--------------------  output !  --------------------\n",
      "gen_images torch.Size([5, 2, 3, 160, 320])\n",
      "transformed_images torch.Size([5, 2, 7, 3, 160, 320])\n",
      "masks torch.Size([5, 2, 7, 1, 160, 320])\n",
      "zs_mu_enc torch.Size([5, 2, 8])\n",
      "zs_log_sigma_sq_enc torch.Size([5, 2, 8])\n",
      "gen_images_enc torch.Size([5, 2, 3, 160, 320])\n",
      "transformed_images_enc torch.Size([5, 2, 7, 3, 160, 320])\n",
      "masks_enc torch.Size([5, 2, 7, 1, 160, 320])\n",
      "gen_images_samples torch.Size([5, 2, 3, 160, 320, 8])\n",
      "gen_images_samples_avg torch.Size([5, 2, 3, 160, 320])\n"
     ]
    }
   ],
   "source": [
    "# 测试 savpcell\n",
    "# liyi, 2019/6/1\n",
    "\n",
    "import functools\n",
    "import itertools\n",
    "import collections\n",
    "import os\n",
    "import re\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import OrderedDict\n",
    "from tensorflow.contrib.training import HParams\n",
    "from video_prediction.utils import util\n",
    "from video_prediction.utils.max_sv import spectral_normed_weight\n",
    "from video_prediction.layers.conv import Conv2d, Conv3d\n",
    "from video_prediction.layers.convLSTM import ConvLSTMCell\n",
    "from video_prediction.models.modules import Prior, Posterior, Dense, Encoder\n",
    "from video_prediction.utils.util import maybe_pad_or_slice\n",
    "from video_prediction.models.savp_model import SAVPCell\n",
    "\n",
    "### 编写于 5/23 6/1\n",
    "class GeneratorGivenZ(nn.Module):\n",
    "    ### inputs 是一个 dict 5/23\n",
    "    ### items() = ['images':NCHW, 'zs':(N,nz)] 5/23\n",
    "    def __init__(self, input_shape, mode, hparams):\n",
    "        super(GeneratorGivenZ, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.image_shape = list(input_shape['images'][2:])\n",
    "        self.time_length = hparams.sequence_length - 1\n",
    "        self.hparams = hparams\n",
    "        self.mode = mode\n",
    "        \n",
    "        savp_input_shape = {}\n",
    "        savp_input_shape['images'] = list(input_shape['images'][1:])\n",
    "        savp_input_shape['zs'] = list(input_shape['zs'][1:])\n",
    "        #print('GeneratorGivenZ: savp_input_shape  ',savp_input_shape)\n",
    "        self.savpcell = SAVPCell(savp_input_shape, mode, hparams)\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        inputs = {name: maybe_pad_or_slice(input, self.hparams.sequence_length - 1)\n",
    "              for name, input in inputs.items()}\n",
    "        batch_size = inputs['images'].shape[1]\n",
    "        #print('GeneratorGivenZ inputs[images]  ',inputs['images'].shape)\n",
    "        #print('GeneratorGivenZ batch_size',batch_size)\n",
    "        #print('-'*20)\n",
    "        #print('GeneratorGivenZ input : sequence_length  ',self.hparams.sequence_length)\n",
    "        #for k,v in inputs.items():\n",
    "        #    print(k,v.shape)\n",
    "        ### initial state 6/1\n",
    "        states = {}\n",
    "        states['time'] = 0\n",
    "        states['gen_image'] = torch.zeros(self.image_shape)\n",
    "        states['last_images'] = [inputs['images'][0]] * self.hparams.last_frames\n",
    "        #print(self.savpcell.rnn_z_state_sizes)\n",
    "        rnn_z_state_size = [batch_size] + self.savpcell.rnn_z_state_sizes\n",
    "        #print(self.savpcell.rnn_z_state_sizes)\n",
    "        conv_rnn_state_size = copy.deepcopy(self.savpcell.conv_rnn_state_sizes)  ### deepcopy()!!!! 6/3\n",
    "        #print('GeneratorGivenZ : self.savpcell.con_rnn_state_sizes[0]  ', self.savpcell.conv_rnn_state_sizes[0])\n",
    "        #print('GeneratorGivenZ : con_rnn_state_sizes[0]  ', conv_rnn_state_size[0])\n",
    "        for i in range(len(conv_rnn_state_size)):\n",
    "            conv_rnn_state_size[i] = [batch_size] + conv_rnn_state_size[i]\n",
    "        #print('GeneratorGivenZ : self.savpcell.con_rnn_state_sizes[0]  ',self.savpcell.conv_rnn_state_sizes[0])\n",
    "        states['rnn_z_state'] = (torch.zeros(rnn_z_state_size),torch.zeros(rnn_z_state_size))\n",
    "        states['conv_rnn_states'] = [(torch.zeros(conv_rnn_state_sz),torch.zeros(conv_rnn_state_sz))\n",
    "                                    for conv_rnn_state_sz in conv_rnn_state_size]\n",
    "        \n",
    "        #print('GeneratorGivenZ : self.savpcell.con_rnn_state_sizes[0]  ',self.savpcell.conv_rnn_state_sizes[0])\n",
    "        ### 把 savpcell 扩展成 rnn 6/1\n",
    "        outputs = {}\n",
    "        for i in range(self.time_length):\n",
    "            input = {k: v[i] for k, v in inputs.items()}\n",
    "            output, new_states = self.savpcell(input, states, inputs['images'])\n",
    "            #print('GeneratorGivenZ rnn input:  ')   ### 6/2\n",
    "            #for k,v in input.items():\n",
    "            #    print(k, v.shape)\n",
    "            #print('GeneratorGivenZ rnn output:  ') ### 6/2\n",
    "            #for k,v in output.items():\n",
    "            #    print(k, v.shape)\n",
    "            if i==0:\n",
    "                for k in output.keys():\n",
    "                    outputs[k] = [output[k]]\n",
    "            else:\n",
    "                for k in output.keys():\n",
    "                    outputs[k].append(output[k])\n",
    "            states = new_states\n",
    "            \n",
    "        for k,v in outputs.items():\n",
    "            outputs[k] = torch.stack(outputs[k], dim=0)\n",
    "            \n",
    "        print('GeneratorGivenZ outputs')\n",
    "        for k,v in outputs.items():\n",
    "            print(k, v.shape)\n",
    "        print('-'*20)\n",
    "        #print('GeneratorGivenZ : self.savpcell.con_rnn_state_sizes[0]  ',self.savpcell.conv_rnn_state_sizes)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "### 编写于 5/23\n",
    "### 待测试。。。 5/23\n",
    "class Generator(nn.Module):\n",
    "    ### inputs DNCHW 5/23\n",
    "    def __init__(self, input_shape, mode, hparams):\n",
    "        super(Generator, self).__init__()\n",
    "        self.mode = mode\n",
    "        self.hparams = hparams\n",
    "        self.input_shape = input_shape\n",
    "        self.batch_size = input_shape[1]\n",
    "        \n",
    "        self.zs_shape = [hparams.sequence_length - 1, self.batch_size, hparams.nz]\n",
    "        self.encoder = Posterior(input_shape, hparams)\n",
    "        if hparams.learn_prior:\n",
    "            self.prior = Prior(input_shape, hparams)\n",
    "        else:\n",
    "            self.prior = None\n",
    "        posterior_shape = {}\n",
    "        posterior_shape['images'] = input_shape\n",
    "        posterior_shape['zs'] = [input_shape[0]-1, input_shape[1], hparams.nz]\n",
    "        self.generator = GeneratorGivenZ(input_shape=posterior_shape, mode=mode, hparams=hparams)  ### 待完成 5/23\n",
    "        \n",
    "    def forward(self, images):\n",
    "        inputs = {}\n",
    "        inputs['images'] = images\n",
    "        if self.hparams.nz == 0:\n",
    "            outputs = self.generator(inputs)\n",
    "        else:\n",
    "            ### encoder 生成 posterior  5/23\n",
    "            outputs_posterior = self.encoder(inputs['images'])\n",
    "            #print(outputs_posterior)\n",
    "            eps = torch.randn(self.zs_shape)\n",
    "            zs_posterior = outputs_posterior['zs_mu'] + \\\n",
    "                    torch.sqrt(torch.exp(outputs_posterior['zs_log_sigma_sq'])) * eps\n",
    "            inputs_posterior = inputs\n",
    "            inputs_posterior['zs'] = zs_posterior\n",
    "            print('Generator : inputs_posterior')\n",
    "            for k, v in inputs_posterior.items():\n",
    "                print(k, v.shape)\n",
    "            print('-'*20)\n",
    "            \n",
    "            ### 生成 prior 5/23\n",
    "            if self.hparams.learn_prior:\n",
    "                outputs_prior = self.prior(inputs['images'])\n",
    "                eps = torch.randn(self.zs_shape)\n",
    "                zs_prior = outputs_prior['zs_mu'] + \\\n",
    "                    torch.sqrt(torch.exp(outputs_prior['zs_log_sigma_sq'])) * eps\n",
    "            else:\n",
    "                outputs_prior = {}\n",
    "                zs_prior = torch.randn([self.hparams.sequence_length - self.hparams.context_frames] + \\\n",
    "                                       self.zs_shape[1:])\n",
    "                zs_prior = torch.cat([zs_posterior[:self.hparams.context_frames - 1], zs_prior], dim=0)\n",
    "            inputs_prior = inputs\n",
    "            inputs_prior['zs'] = zs_prior\n",
    "            print('Generator : inputs_prior')\n",
    "            for k, v in inputs_prior.items():\n",
    "                print(k, v.shape)\n",
    "            print('-'*20)\n",
    "            \n",
    "            ### posterior 和 images 交给 generator 5/23\n",
    "            gen_outputs_posterior = self.generator(inputs_posterior)\n",
    "            print('-'*20,' gen posterior complete ','-'*20)\n",
    "            gen_outputs = self.generator(inputs_prior)\n",
    "            print('-'*20,' gen prior complete ','-'*20)\n",
    "            \n",
    "            # rename tensors to avoid name collisions\n",
    "            output_prior = collections.OrderedDict([(k + '_prior', v) for k, v in outputs_prior.items()])\n",
    "            outputs_posterior = collections.OrderedDict([(k + '_enc', v) for k, v in outputs_posterior.items()])\n",
    "            gen_outputs_posterior = collections.OrderedDict([(k + '_enc', v) for k, v in gen_outputs_posterior.items()])\n",
    "            \n",
    "            outputs = [output_prior, gen_outputs, outputs_posterior, gen_outputs_posterior]\n",
    "            total_num_outputs = sum([len(output) for output in outputs])\n",
    "            ### 整合 5/23\n",
    "            outputs = collections.OrderedDict(itertools.chain(*[output.items() for output in outputs]))\n",
    "            assert len(outputs) == total_num_outputs  # ensure no output is lost because of repeated keys\n",
    "\n",
    "            ### 根据 prior 生成多个随机抽样 5/23\n",
    "            ### num_samples 是采样的个数 5/23\n",
    "            inputs_samples = {\n",
    "                name: input[:, None].repeat([1, self.hparams.num_samples]+[1]*(len(input.shape) - 1))\n",
    "                for name, input in inputs.items()}\n",
    "            zs_samples_shape = [self.hparams.sequence_length - 1,\n",
    "                                self.hparams.num_samples,\n",
    "                                self.batch_size,\n",
    "                                self.hparams.nz]\n",
    "            print('Generator : inputs_samples')\n",
    "            for k,v in inputs_samples.items():\n",
    "                print(k, v.shape)\n",
    "            print('-'*20)\n",
    "            if self.hparams.learn_prior:\n",
    "                eps = torch.randn(zs_samples_shape)\n",
    "                zs_prior_samples = (outputs_prior['zs_mu'][:, None] +\n",
    "                                torch.sqrt(torch.exp(outputs_prior['zs_log_sigma_sq']))[:, None] * eps)\n",
    "            else:\n",
    "                zs_prior_samples = torch.randn(\n",
    "                    [self.hparams.sequence_length - self.hparams.context_frames] + zs_samples_shape[1:])\n",
    "                zs_prior_samples = torch.cat(\n",
    "                    [zs_posterior[:self.hparams.context_frames - 1][:, None].repeat(\n",
    "                                  [1, self.hparams.num_samples, 1, 1]),\n",
    "                     zs_prior_samples], dim=0)\n",
    "            inputs_prior_samples = dict(inputs_samples)\n",
    "            inputs_prior_samples['zs'] = zs_prior_samples\n",
    "            \n",
    "            ### 第1，2个维度压平 5/23\n",
    "            inputs_prior_samples = {name: torch.flatten(input, start_dim=1, end_dim=2)\n",
    "                                    for name, input in inputs_prior_samples.items()}\n",
    "            gen_outputs_samples = self.generator(inputs_prior_samples)\n",
    "            gen_images_samples = gen_outputs_samples['gen_images']\n",
    "            ### 再恢复出前两个维度 5/23\n",
    "            print('Generator samples')\n",
    "            print('Generator : gen_images_Samples  ',gen_images_samples.shape)\n",
    "            gen_images_samples = torch.stack(gen_images_samples.chunk(self.hparams.num_samples, dim=1), dim=-1)\n",
    "            gen_images_samples_avg = torch.mean(gen_images_samples, dim=-1)\n",
    "            outputs['gen_images_samples'] = gen_images_samples\n",
    "            outputs['gen_images_samples_avg'] = gen_images_samples_avg\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "images = torch.tensor(np.transpose(inputs,[1,0,4,2,3]))  # to DNCHW 5/22\n",
    "myG = Generator(images.shape, 'train', parsed_hparams)\n",
    "out = myG(images)\n",
    "print('-'*20, ' output ! ','-'*20)\n",
    "for k,v in out.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "a = torch.Size([2,3,3])\n",
    "print(len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 30, 160, 320, 3)\n",
      "apply_kernels: input   (6, 30, 160, 320, 3)\n",
      "apply_cnda_kernels: input   torch.Size([6, 160, 320, 3])\n",
      "apply_cnda_kernels: output   1 torch.Size([1, 6, 3, 160, 320])\n",
      "apply_kernels: output   1 torch.Size([1, 6, 3, 160, 320])\n"
     ]
    }
   ],
   "source": [
    "### 测试 sav_model.py 中的 apply kernel\n",
    "### liyi, 2019/6/2\n",
    "\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "from video_prediction.models.savp_model import apply_cdna_kernels, identity_kernel\n",
    "\n",
    "path = '/home/liyi/video-pred/pytorch_video_pred/data/comma/train/'\n",
    "# print(sorted(os.listdir(path))[0])\n",
    "batch_size = 6\n",
    "idx = range(batch_size)\n",
    "files = ['%010d.h5'% i for i in idx]\n",
    "inputs = []\n",
    "for file in files:\n",
    "    f = h5py.File(path+file, 'r')\n",
    "    sample = dict(f)['image'].value.astype(np.float32)\n",
    "    inputs.append(sample)\n",
    "    f.close()\n",
    "    \n",
    "inputs = np.array(inputs)\n",
    "\n",
    "def apply_kernels(image, kernels, dilation_rate=(1, 1)):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        image: A 4-D tensor of shape\n",
    "            `[batch, in_height, in_width, in_channels]`.\n",
    "        kernels: A 4-D tensor of shape\n",
    "            `[batch, kernel_size[0], kernel_size[1], num_transformed_images]` or\n",
    "\n",
    "    Returns:\n",
    "        A list of `num_transformed_images` 4-D tensors, each of shape\n",
    "            `[batch, in_height, in_width, in_channels]`.\n",
    "    \"\"\"\n",
    "    print('apply_kernels: input  ', inputs.shape)\n",
    "    if isinstance(image, list):\n",
    "        image_list = image\n",
    "        kernels_list = torch.chunk(kernels, len(image_list), dim=-1)\n",
    "        outputs = []\n",
    "        for image, kernels in zip(image_list, kernels_list):\n",
    "            outputs.extend(apply_kernels(image, kernels))\n",
    "    else:\n",
    "        outputs = apply_cdna_kernels(image, kernels, dilation_rate=dilation_rate)\n",
    "    print('apply_kernels: output  ', len(outputs),outputs[0].shape)\n",
    "    return outputs\n",
    "\n",
    "kernel = identity_kernel((5,5))[None, :, :, None].repeat([6,1,1,1])\n",
    "print(inputs.shape)\n",
    "imgs = torch.Tensor(inputs[:,1])\n",
    "out = apply_kernels(imgs, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 6, 256])\n",
      "z_mu torch.Size([28, 6, 8])\n",
      "z_log_sigma_sq torch.Size([28, 6, 8])\n"
     ]
    }
   ],
   "source": [
    "# 测试 prior\n",
    "# from module.py\n",
    "# liyi,2019/5/22\n",
    "\n",
    "import functools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from video_prediction.utils.max_sv import spectral_normed_weight\n",
    "from video_prediction.layers.conv import Conv2d, Conv3d\n",
    "from video_prediction.models.modules import Encoder,Dense\n",
    "\n",
    "class Prior(nn.Module):\n",
    "    ### 改写自savp_model.py prior_fn 5/16\n",
    "    ### inputs应当是 NDCHW 5/22\n",
    "    def __init__(self, input_shape, hparams):\n",
    "        super(Prior, self).__init__()\n",
    "        self.hparams = hparams\n",
    "        self.input_shape = list(input_shape)\n",
    "        ## inputs concat 过程中只取了 inputs[:self.hparams.context_frames - 1] 5/22\n",
    "        self.concat_shape = [(hparams.context_frames-1)%self.input_shape[0]]+ \\\n",
    "                            [self.input_shape[1]]+ \\\n",
    "                            [self.input_shape[-3]*2]+ \\\n",
    "                            self.input_shape[-2:]\n",
    "        encoder_input_shape = [np.prod(self.concat_shape[0:2])]+self.concat_shape[2:]\n",
    "        self.encoder = Encoder(input_shape=encoder_input_shape,\n",
    "                               nef=hparams.nef,\n",
    "                               n_layers=hparams.n_layers)### input_shape需要根据hparmas修改 5/16\n",
    "        \n",
    "        encoder_out_shape = self.concat_shape[0:2]+ \\\n",
    "                            [hparams.nef * min(4, 2**(hparams.n_layers-1))]\n",
    "        dense_input_shape = encoder_out_shape\n",
    "        dense_input_shape[0] += hparams.sequence_length - hparams.context_frames\n",
    "        dense_input_shape = [np.prod(dense_input_shape[0:2])] + dense_input_shape[2:]\n",
    "        self.dense0 = Dense(input_shape=dense_input_shape, units=hparams.nef * 4)\n",
    "        \n",
    "        ### lstm input 应为 (seq_len, batch, input_size) 5/22\n",
    "        rnn_input_shape = self.concat_shape[0:2]+[hparams.nef * 4]\n",
    "        if hparams.rnn == 'lstm':\n",
    "            # input_size: The number of expected features in the input x 5/23\n",
    "            # inputs: input, (h_0, c_0)\n",
    "            #         input of shape=(seq_len, batch, input_size) 5/23\n",
    "            # outputs: output, (h_n, c_n) 5/23\n",
    "            #          output of shape (seq_len, batch, num_directions * hidden_size) 5/23\n",
    "            self.rnn = nn.LSTM(input_size=hparams.nef * 4, hidden_size=hparams.nef * 4)\n",
    "        elif hparams.rnn == 'gru':\n",
    "            self.rnn = nn.GRU(input_size=hparams.nef * 4, hidden_size=hparams.nef * 4)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        rnn_output_shape = self.concat_shape[0:2]+[hparams.nef * 4]\n",
    "        dense_input_shape = [np.prod(rnn_output_shape[0:2])] + rnn_output_shape[2:]\n",
    "        self.dense1 = Dense(input_shape=dense_input_shape, units=hparams.nz)  ### input_shape要改 5/16\n",
    "        self.dense2 = Dense(input_shape=dense_input_shape, units=hparams.nz)  ### input_shape要改 5/16\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        ### inputs应当是 NDCHW 5/22\n",
    "        outputs = {}\n",
    "        ### 将连续的两帧图片在channel维度上级联 5/16\n",
    "        ### context_frams 需要根据 ... 5/16\n",
    "        inputs = torch.cat([inputs[:self.hparams.context_frames - 1],\n",
    "                            inputs[1:self.hparams.context_frames]], dim=-3)  \n",
    "        inputs = inputs.reshape([-1]+list(inputs.shape[-3:]))  ### 变为 NCHW 5/23\n",
    "        ### 加入 action uncompleted ... \n",
    "        h = self.encoder(inputs)['output']\n",
    "        h = h.reshape(self.concat_shape[0:2]+[-1])\n",
    "        \n",
    "        h_zeros = torch.zeros(size=[self.hparams.sequence_length-self.hparams.context_frames]+list(h.shape[1:]))\n",
    "        h = torch.cat([h, h_zeros], dim=0)\n",
    "        \n",
    "        h = h.reshape([-1, h.size(-1)])\n",
    "        h = self.dense0(h)\n",
    "        h = h.reshape(self.concat_shape[0:2]+[-1])\n",
    "        h = self.rnn(h)[0]\n",
    "        # print(len(h))      # 2 5/23\n",
    "        # print(h[0].shape)  # torch.Size([28, 6, 256]) 5/23\n",
    "        # print(len(h[1]))   # 2 5/23\n",
    "        # for k in h[1]:\n",
    "            # print(k.shape)\n",
    "            # torch.Size([1, 6, 256]) 5/23\n",
    "            # torch.Size([1, 6, 256])\n",
    "        \n",
    "        h = h.reshape([-1, h.size(-1)])\n",
    "        z_mu = self.dense1(h).reshape(self.concat_shape[0:2]+[-1])\n",
    "        outputs['z_mu'] = z_mu\n",
    "        z_log_sigma_sq = self.dense2(h).reshape(self.concat_shape[0:2]+[-1])\n",
    "        z_log_sigma_sq = torch.clamp(z_log_sigma_sq, -10,10)\n",
    "        outputs['z_log_sigma_sq'] = z_log_sigma_sq\n",
    "        return outputs\n",
    "        \n",
    "images = torch.tensor(np.transpose(inputs,[1,0,4,2,3]))  # to DNCHW 5/22\n",
    "net = Prior(input_shape=images.shape, hparams=parsed_hparams)\n",
    "output = net.forward(images)\n",
    "for k in output.keys():\n",
    "    # z_mu torch.Size([28, 6, 8])   5/23\n",
    "    # z_log_sigma_sq torch.Size([28, 6, 8])\n",
    "    print(k, output[k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[180, 256]\n",
      "torch.Size([174, 6, 160, 320])\n",
      "z_mu torch.Size([29, 6, 8])\n",
      "z_log_sigma_sq torch.Size([29, 6, 8])\n"
     ]
    }
   ],
   "source": [
    "# 测试 posterior\n",
    "# from modules.py\n",
    "# liyi,2019/5/22\n",
    "# input 为 DNCHW 5/22\n",
    "# output 为 D-1,N,nz[=8] 5/22\n",
    "\n",
    "import functools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from video_prediction.utils.max_sv import spectral_normed_weight\n",
    "from video_prediction.layers.conv import Conv2d, Conv3d\n",
    "from video_prediction.models.modules import Encoder,Dense\n",
    "\n",
    "class Posterior(nn.Module):\n",
    "    ### 改写自savp_model.py posterior_fn 5/15\n",
    "    ### input 为 DNCHW 5/19\n",
    "    ### output 为 D-1,N,nz[=8] 5/21\n",
    "    def __init__(self, input_shape, hparams):\n",
    "        super(Posterior, self).__init__()\n",
    "        self.input_shape = list(input_shape)\n",
    "        self.use_e_rnn = hparams.use_e_rnn  ### 默认false 5/19\n",
    "        \n",
    "        #input_shape = list(input_shape)\n",
    "        #input_shape=[np.prod(input_shape[0:2])]+[input_shape[-3]*2]+input_shape[-2:]\n",
    "        #print(input_shape)\n",
    "        self.encoder = Encoder(input_shape=[np.prod(self.input_shape[0:2])]+[self.input_shape[-3]*2]+self.input_shape[-2:],\n",
    "                               nef=hparams.nef, n_layers=hparams.n_layers)\n",
    "        out_shape = [np.prod(self.input_shape[0:2]),\n",
    "                     hparams.nef * min(4, 2**(hparams.n_layers-1))]\n",
    "        print(out_shape)\n",
    "        self.dense1 = Dense(input_shape=out_shape, units=hparams.nz)  ### input_shape要改 5/15\n",
    "        self.dense2 = Dense(input_shape=out_shape, units=hparams.nz)  ### input_shape要改 5/15\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        ### inputs应当是 NDCHW 5/16\n",
    "        outputs = {}\n",
    "        inputs = torch.cat([inputs[:-1], inputs[1:]], dim=-3) ### 将连续的两帧图片在channel维度上级联 5/16\n",
    "        inputs = inputs.reshape([-1]+list(inputs.shape[-3:]))  ### 变为 NCHW 5/22\n",
    "        print(inputs.shape)\n",
    "        ### 加入 action uncompleted ... \n",
    "        h = self.encoder(inputs)['output']\n",
    "        if self.use_e_rnn:\n",
    "            h = self.dense0(h)\n",
    "            h = self.rnn(h)\n",
    "        z_mu = self.dense1(h).reshape([self.input_shape[0]-1]+[self.input_shape[1]]+[-1])\n",
    "        outputs['z_mu'] = z_mu\n",
    "        z_log_sigma_sq = self.dense2(h).reshape([self.input_shape[0]-1]+[self.input_shape[1]]+[-1])\n",
    "        z_log_sigma_sq = torch.clamp(z_log_sigma_sq, -10,10)\n",
    "        outputs['z_log_sigma_sq'] = z_log_sigma_sq\n",
    "        return outputs\n",
    "    \n",
    "images = torch.tensor(np.transpose(inputs,[1,0,4,2,3]))  # to DNCHW 5/22\n",
    "net = Posterior(input_shape=images.shape, hparams=parsed_hparams)\n",
    "output = net.forward(images)\n",
    "for k in output.keys():\n",
    "    print(k, output[k].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 30, 320, 3, 160])\n",
      "conv:  torch.Size([30, 256, 1, 20])\n",
      "pool:  torch.Size([30, 256, 1, 1])\n",
      "torch.Size([30, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liyi/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "# 测试encoder\n",
    "# from modules.py\n",
    "# liyi，2019/5/22\n",
    "\n",
    "import functools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from video_prediction.utils.max_sv import spectral_normed_weight\n",
    "from video_prediction.layers.conv import Conv2d, Conv3d\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    ### conv2d的in_channels是否为3存疑 5/8\n",
    "    ### nef 为 encoder 的 filter 个数 5/9\n",
    "    ### conv2d 要求 input_shape = NCHW\n",
    "    ### input_shape = (ND/DN,CHW) 5/22\n",
    "    ### output_shape = (ND/DN,min(2**(n_layers-1),4)),只有三个维度 5/21\n",
    "    def __init__(self, input_shape, nef=64, n_layers=3):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.conv = {}\n",
    "        self.norm = {}\n",
    "        self.conv0 = nn.Conv2d(in_channels=self.input_shape[-3], \n",
    "                               out_channels=nef, kernel_size=4, stride=2, padding=(1,1))\n",
    "        def make_sequence(in_channel, i):\n",
    "            out_channel = nef * min(2**i, 4)\n",
    "            return [nn.Conv2d(\n",
    "                        in_channels=in_channel,\n",
    "                        out_channels=out_channel, \n",
    "                        kernel_size=4, stride=2,\n",
    "                        padding=(1,1)),\n",
    "                      nn.InstanceNorm2d(\n",
    "                        num_features=out_channel,\n",
    "                        eps=1e-6)], out_channel\n",
    "        \n",
    "        self.model_list = nn.ModuleList()\n",
    "        in_channel = nef\n",
    "        for i in range(1, n_layers):\n",
    "            sequence, in_channel = make_sequence(in_channel, i)\n",
    "            self.model_list += sequence\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        ### inputs 应当是 NCHW 5/8\n",
    "        outputs = {}\n",
    "        output = self.conv0(inputs)\n",
    "        output = F.leaky_relu(output, negative_slope=0.2)\n",
    "        n = 0\n",
    "        outputs['encoder_%d'%n] = output     ### for visualization 5/8\n",
    "        for model in self.model_list:\n",
    "            n += 1\n",
    "            output = model(output)\n",
    "            output = F.leaky_relu(output, negative_slope=0.2)\n",
    "            outputs['encoder_%d'%n] = output\n",
    "        print('conv: ',output.shape)\n",
    "        output = F.avg_pool2d(output, output.shape[2:])\n",
    "        print('pool: ',output.shape)\n",
    "        output.squeeze_(dim=-2)  # 对HW两个维度squeeze\n",
    "        output.squeeze_(dim=-1)\n",
    "        outputs['output'] = output\n",
    "        return outputs\n",
    "    \n",
    "inputs = torch.tensor(np.transpose(inputs,axes=[0,1,4,2,3])) # NDHWC to NDCHW 5/22\n",
    "print(inputs.shape)\n",
    "net = Encoder(input_shape=inputs[0].shape)\n",
    "outputs = net(inputs[0])\n",
    "print(outputs['output'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n",
      "[  2  12 160 320   3]\n",
      "[ -1 160 320   3]\n",
      "(4,)\n",
      "[ -1 160 320   3]\n",
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "ndims = 4\n",
    "x = tf.zeros([2,12,160,320,3])\n",
    "\n",
    "sess = tf.Session()\n",
    "print(x1.get_shape())\n",
    "\n",
    "shape = tf.shape(x)\n",
    "print(sess.run(shape))\n",
    "\n",
    "x1 = tf.concat([[-1], shape[-(ndims-1):]], axis=0)\n",
    "print(sess.run(x1))\n",
    "print(x1.get_shape())\n",
    "\n",
    "x1.set_shape([ndims])\n",
    "print(sess.run(x1))\n",
    "print(x1.get_shape())\n",
    "\n",
    "#sess = tf.Session()\n",
    "#print(sess.run(tf.shape(x1), feed_dict={x1:[0,1,2,3]}))\n",
    "#x1.set_shape([2,2])\n",
    "#print(sess.run(tf.shape(x1)))\n",
    "#print(sess.run(tf.shape(x1), feed_dict={x1:[[0,1],[2,3]]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 4],\n",
      "        [2, 3, 4]])\n",
      "tensor([[1, 2],\n",
      "        [4, 2],\n",
      "        [3, 4]])\n",
      "torch.Size([3, 2])\n",
      "tensor([[1, 2, 4, 2, 3, 4]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([[1,2,4],[2,3,4]])\n",
    "b = a.reshape([-1,2])\n",
    "print(a)\n",
    "print(b)\n",
    "print(b.shape)\n",
    "b = a.view([1,6])\n",
    "print(b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
