{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 5, 5])\n",
      "torch.Size([1, 16, 5, 5])\n",
      "torch.Size([1, 2, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# With square kernels and equal stride\n",
    "filters = torch.randn(8,4,3,3)\n",
    "inputs = torch.randn(1,4,5,5)\n",
    "out = F.conv2d(inputs, filters, padding=1)\n",
    "print(out.shape)\n",
    "filters = torch.randn(16,2,3,3)\n",
    "out = F.conv2d(inputs, filters, groups=2, padding=1)\n",
    "print(out.shape)\n",
    "out = torch.split(out,2,dim=1)\n",
    "#for o in out:\n",
    "#    print(o.shape)\n",
    "oo = sum(out)\n",
    "print(oo.shape)\n",
    "#print(out[0].shape)\n",
    "#o = torch.stack(out[0],dim=0)\n",
    "#print(o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 30, 160, 320, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liyi/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# 读取一个 h5 图片数据集，便于后面使用\n",
    "# liyi，2019/5/19\n",
    "\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "path = '/home/liyi/video-pred/pytorch_video_pred/data/comma/train/'\n",
    "# print(sorted(os.listdir(path))[0])\n",
    "batch_size = 6\n",
    "idx = range(batch_size)\n",
    "files = ['%010d.h5'% i for i in idx]\n",
    "inputs = []\n",
    "for file in files:\n",
    "    f = h5py.File(path+file, 'r')\n",
    "    sample = dict(f)['image'].value.astype(np.float32)\n",
    "    inputs.append(sample)\n",
    "    f.close()\n",
    "    \n",
    "inputs = np.array(inputs)\n",
    "print(inputs.shape)  # NDHWC 5/22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HParams([('context_frames', -1), ('repeat', 1), ('sequence_length', -1)])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取超参数，方便后面调试\n",
    "# from base_model.py\n",
    "# liyi，2019/5/19\n",
    "\n",
    "from tensorflow.contrib.training import HParams\n",
    "\n",
    "def get_hparams(hparams_dict=None):\n",
    "    hparams = dict(\n",
    "        context_frames=-1,\n",
    "        sequence_length=-1,\n",
    "        repeat=1,\n",
    "    )\n",
    "    hparams.update(hparams_dict or {})\n",
    "    return HParams(**hparams)\n",
    "\n",
    "get_hparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liyi/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 30, 160, 320, 3)\n"
     ]
    }
   ],
   "source": [
    "# 读取一个 h5 图片数据集，便于后面使用\n",
    "# liyi，2019/5/19\n",
    "\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "path = '/home/liyi/video-pred/pytorch_video_pred/data/comma/train/'\n",
    "# print(sorted(os.listdir(path))[0])\n",
    "batch_size = 6\n",
    "idx = range(batch_size)\n",
    "files = ['%010d.h5'% i for i in idx]\n",
    "inputs = []\n",
    "for file in files:\n",
    "    f = h5py.File(path+file, 'r')\n",
    "    sample = dict(f)['image'].value.astype(np.float32)\n",
    "    inputs.append(sample)\n",
    "    f.close()\n",
    "    \n",
    "inputs = np.array(inputs)\n",
    "print(inputs.shape)  # NDHWC 5/22\n",
    "\n",
    "\n",
    "# 获取超参数，方便后面调试\n",
    "# from base_model.py\n",
    "# liyi，2019/5/19\n",
    "\n",
    "from tensorflow.contrib.training import HParams\n",
    "\n",
    "def get_hparams(hparams_dict=None):\n",
    "    hparams = dict(\n",
    "        context_frames=-1,\n",
    "        sequence_length=-1,\n",
    "        repeat=1,\n",
    "    )\n",
    "    hparams.update(hparams_dict or {})\n",
    "    return HParams(**hparams)\n",
    "\n",
    "\n",
    "# 为下面测试 posterior 准备超参数 5/22\n",
    "hparams = dict(\n",
    "    dataset='bair',\n",
    "    input_dir='/home/liyi/video-pred/video_prediction/data/comma_m/train',\n",
    "    model='savp',\n",
    "    model_hparams_dict='hparams/bair_action_free/ours_savp/model_hparams.json',\n",
    "    \n",
    "    l1_weight=1.0,\n",
    "    l2_weight=0.0,\n",
    "    n_layers=3,  # 3改为5 5/21\n",
    "    ndf=32,\n",
    "    norm_layer='instance',\n",
    "    use_same_discriminator=False,\n",
    "    ngf=32,\n",
    "    downsample_layer='conv_pool2d',\n",
    "    upsample_layer='upsample_conv2d',\n",
    "    activation_layer='relu',  # for generator only\n",
    "    transformation='cdna',\n",
    "    kernel_size=(5, 5),\n",
    "    dilation_rate=(1, 1),\n",
    "    where_add='all',\n",
    "    use_tile_concat=True,\n",
    "    learn_initial_state=False,\n",
    "    rnn='lstm',\n",
    "    conv_rnn='lstm',\n",
    "    conv_rnn_norm_layer='instance',\n",
    "    num_transformed_images=4,\n",
    "    last_frames=1,\n",
    "    prev_image_background=True,\n",
    "    first_image_background=True,\n",
    "    last_image_background=False,\n",
    "    last_context_image_background=False,\n",
    "    context_images_background=False,\n",
    "    generate_scratch_image=True,\n",
    "    dependent_mask=True,\n",
    "    schedule_sampling='inverse_sigmoid',\n",
    "    schedule_sampling_k=900.0,\n",
    "    schedule_sampling_steps=(0, 100000),\n",
    "    use_e_rnn=False,\n",
    "    learn_prior=False,\n",
    "    nz=8,\n",
    "    num_samples=8,\n",
    "    nef=64,   ### 64改为32 5/21\n",
    "    use_rnn_z=True,\n",
    "    ablation_conv_rnn_norm=False,\n",
    "    ablation_rnn=False,\n",
    "    \n",
    "    ### from base_model.py VideoPredictionModel 6/1\n",
    "    batch_size=6,\n",
    "    lr=0.001,\n",
    "    end_lr=0.0,\n",
    "    decay_steps=(200000, 300000),\n",
    "    lr_boundaries=(0,),\n",
    "    max_steps=300000,\n",
    "    beta1=0.9,\n",
    "    beta2=0.999,\n",
    "    context_frames=2,     ### -1改为2\n",
    "    sequence_length=30,   ### -1改为\n",
    "    clip_length=10,\n",
    "    vgg_cdist_weight=0.0,\n",
    "    feature_l2_weight=0.0,\n",
    "    ae_l2_weight=0.0,\n",
    "    state_weight=0.0,\n",
    "    tv_weight=0.0,\n",
    "    image_sn_gan_weight=0.0,\n",
    "    image_sn_vae_gan_weight=0.0,\n",
    "    images_sn_gan_weight=0.0,\n",
    "    images_sn_vae_gan_weight=0.0,\n",
    "    video_sn_gan_weight=0.0,\n",
    "    video_sn_vae_gan_weight=0.0,\n",
    "    gan_feature_l2_weight=0.0,\n",
    "    gan_feature_cdist_weight=0.0,\n",
    "    vae_gan_feature_l2_weight=0.0,\n",
    "    vae_gan_feature_cdist_weight=0.0,\n",
    "    gan_loss_type='LSGAN',\n",
    "    joint_gan_optimization=False,\n",
    "    kl_weight=0.0,\n",
    "    kl_anneal='linear',\n",
    "    kl_anneal_k=-1.0,\n",
    "    kl_anneal_steps=(50000, 100000),\n",
    "    z_l1_weight=0.0,\n",
    ")\n",
    "\n",
    "parsed_hparams = get_hparams(hparams_dict=hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'images': [30, 3, 160, 320], 'zs': [29, 8]}\n",
      "images torch.Size([30, 6, 3, 160, 320])\n",
      "zs torch.Size([29, 6, 8])\n",
      "--------------------\n",
      "images torch.Size([30, 6, 3, 160, 320])\n",
      "zs torch.Size([29, 6, 8])\n",
      "--------------------\n",
      "torch.Size([6, 8])\n",
      "torch.Size([30, 8])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input batch size 6 doesn't match hidden[0] batch size 30",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-76fc9fa8c843>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# to DNCHW 5/22\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0mmyG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparsed_hparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-76fc9fa8c843>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;31m### posterior 和 images 交给 generator 5/23\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0mgen_outputs_posterior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_posterior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0mgen_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_prior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-76fc9fa8c843>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavpcell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'images'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/video-pred/pytorch_video_pred/video_prediction/models/savp_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, states, all_images)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'zs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rnn_z_state'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm_z\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'zs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rnn_z_state'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m         \u001b[0mrnn_z_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m### hx,cx 对应 rnn_z, rnn_z_state 5/29\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0mstate_action_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhx\u001b[0m    \u001b[0;31m### state_action_z = (N, hidden_size=hparams.nz) 5/29\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_zeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'[0]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'[1]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m         return _VF.lstm_cell(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_hidden\u001b[0;34m(self, input, hx, hidden_label)\u001b[0m\n\u001b[1;32m    570\u001b[0m             raise RuntimeError(\n\u001b[1;32m    571\u001b[0m                 \"Input batch size {} doesn't match hidden{} batch size {}\".format(\n\u001b[0;32m--> 572\u001b[0;31m                     input.size(0), hidden_label, hx.size(0)))\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input batch size 6 doesn't match hidden[0] batch size 30"
     ]
    }
   ],
   "source": [
    "# 测试 savpcell\n",
    "# liyi, 2019/6/1\n",
    "\n",
    "import functools\n",
    "import itertools\n",
    "import collections\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import OrderedDict\n",
    "from tensorflow.contrib.training import HParams\n",
    "from video_prediction.utils import util\n",
    "from video_prediction.utils.max_sv import spectral_normed_weight\n",
    "from video_prediction.layers.conv import Conv2d, Conv3d\n",
    "from video_prediction.layers.convLSTM import ConvLSTMCell\n",
    "from video_prediction.models.modules import Prior, Posterior, Dense, Encoder\n",
    "from video_prediction.utils.util import maybe_pad_or_slice\n",
    "from video_prediction.models.savp_model import SAVPCell\n",
    "\n",
    "### 编写于 5/23 6/1\n",
    "class GeneratorGivenZ(nn.Module):\n",
    "    ### inputs 是一个 dict 5/23\n",
    "    ### keys() = ['images', 'zs'] 5/23\n",
    "    def __init__(self, input_shape, mode, hparams):\n",
    "        super(GeneratorGivenZ, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.image_shape = list(input_shape['images'][2:])\n",
    "        self.time_length = input_shape['images'][0]\n",
    "        self.hparams = hparams\n",
    "        self.mode = mode\n",
    "        \n",
    "        savp_input_shape = {}\n",
    "        savp_input_shape['images'] = list(input_shape['images'][1:])\n",
    "        savp_input_shape['zs'] = list(input_shape['zs'][1:])\n",
    "        print(savp_input_shape)\n",
    "        self.savpcell = SAVPCell(savp_input_shape, mode, hparams)\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        inputs = {name: maybe_pad_or_slice(input, self.hparams.sequence_length - 1)\n",
    "              for name, input in inputs.items()}\n",
    "        ### initial state 6/1\n",
    "        states = {}\n",
    "        states['time'] = 0\n",
    "        states['gen_image'] = torch.zeros(self.image_shape)\n",
    "        states['last_images'] = [inputs['images'][0]] * self.hparams.last_frames\n",
    "        rnn_z_state_sizes = self.savpcell.rnn_z_state_sizes\n",
    "        conv_rnn_state_sizes = self.savpcell.conv_rnn_state_sizes\n",
    "        states['rnn_z_state'] = (torch.zeros(rnn_z_state_sizes),torch.zeros(rnn_z_state_sizes))\n",
    "        states['conv_rnn_states'] = [(torch.zeros(conv_rnn_state_size),torch.zeros(conv_rnn_state_size))\n",
    "                                    for conv_rnn_state_size in conv_rnn_state_sizes]\n",
    "        \n",
    "        ### 把 savpcell 扩展成 rnn 6/1\n",
    "        outputs = {}\n",
    "        for i in range(self.time_length):\n",
    "            input = {k: v[i] for k, v in inputs.items()}\n",
    "            output, new_states = self.savpcell(input, states, inputs['images'])\n",
    "            if i==0:\n",
    "                outputs = output\n",
    "            else:\n",
    "                for k in output.keys():\n",
    "                    outputs[k] = torch.cat([outputs[k],[output[k]]],dim=0)\n",
    "            states = new_states\n",
    "            \n",
    "        return outputs\n",
    "\n",
    "\n",
    "### 编写于 5/23\n",
    "### 待测试。。。 5/23\n",
    "class Generator(nn.Module):\n",
    "    ### inputs DNCHW 5/23\n",
    "    def __init__(self, input_shape, mode, hparams):\n",
    "        super(Generator, self).__init__()\n",
    "        self.mode = mode\n",
    "        self.hparams = hparams\n",
    "        self.input_shape = input_shape\n",
    "        self.batch_size = input_shape[1]\n",
    "        \n",
    "        self.zs_shape = [hparams.sequence_length - 1, self.batch_size, hparams.nz]\n",
    "        self.encoder = Posterior(input_shape, hparams)\n",
    "        if hparams.learn_prior:\n",
    "            self.prior = Prior(input_shape, hparams)\n",
    "        else:\n",
    "            self.prior = None\n",
    "        posterior_shape = {}\n",
    "        posterior_shape['images'] = input_shape\n",
    "        posterior_shape['zs'] = [input_shape[0]-1, input_shape[1], hparams.nz]\n",
    "        self.generator = GeneratorGivenZ(input_shape=posterior_shape, mode=mode, hparams=hparams)  ### 待完成 5/23\n",
    "        \n",
    "    def forward(self, images):\n",
    "        inputs = {}\n",
    "        inputs['images'] = images\n",
    "        if self.hparams.nz == 0:\n",
    "            outputs = self.generator(inputs)\n",
    "        else:\n",
    "            ### encoder 生成 posterior  5/23\n",
    "            outputs_posterior = self.encoder(inputs['images'])\n",
    "            #print(outputs_posterior)\n",
    "            eps = torch.randn(self.zs_shape)\n",
    "            zs_posterior = outputs_posterior['zs_mu'] + \\\n",
    "                    torch.sqrt(torch.exp(outputs_posterior['zs_log_sigma_sq'])) * eps\n",
    "            inputs_posterior = inputs\n",
    "            inputs_posterior['zs'] = zs_posterior\n",
    "            for k, v in inputs_posterior.items():\n",
    "                print(k, v.shape)\n",
    "            print('-'*20)\n",
    "            \n",
    "            ### 生成 prior 5/23\n",
    "            if self.hparams.learn_prior:\n",
    "                outputs_prior = self.prior(inputs['images'])\n",
    "                eps = torch.randn(self.zs_shape)\n",
    "                zs_prior = outputs_prior['zs_mu'] + \\\n",
    "                    torch.sqrt(torch.exp(outputs_prior['zs_log_sigma_sq'])) * eps\n",
    "            else:\n",
    "                outputs_prior = {}\n",
    "                zs_prior = torch.randn([self.hparams.sequence_length - self.hparams.context_frames] + \\\n",
    "                                       self.zs_shape[1:])\n",
    "                zs_prior = torch.cat([zs_posterior[:self.hparams.context_frames - 1], zs_prior], dim=0)\n",
    "            inputs_prior = inputs\n",
    "            inputs_prior['zs'] = zs_prior\n",
    "            for k, v in inputs_prior.items():\n",
    "                print(k, v.shape)\n",
    "            print('-'*20)\n",
    "            \n",
    "            ### posterior 和 images 交给 generator 5/23\n",
    "            gen_outputs_posterior = self.generator(inputs_posterior)\n",
    "            gen_outputs = self.generator(inputs_prior)\n",
    "            \n",
    "            # rename tensors to avoid name collisions\n",
    "            output_prior = collections.OrderedDict([(k + '_prior', v) for k, v in outputs_prior.items()])\n",
    "            outputs_posterior = collections.OrderedDict([(k + '_enc', v) for k, v in outputs_posterior.items()])\n",
    "            gen_outputs_posterior = collections.OrderedDict([(k + '_enc', v) for k, v in gen_outputs_posterior.items()])\n",
    "            \n",
    "            outputs = [output_prior, gen_outputs, outputs_posterior, gen_outputs_posterior]\n",
    "            total_num_outputs = sum([len(output) for output in outputs])\n",
    "            ### 整合 5/23\n",
    "            outputs = collections.OrderedDict(itertools.chain(*[output.items() for output in outputs]))\n",
    "            assert len(outputs) == total_num_outputs  # ensure no output is lost because of repeated keys\n",
    "\n",
    "            ### 根据 prior 生成多个随机抽样 5/23\n",
    "            ### num_samples 是采样的个数 5/23\n",
    "            inputs_samples = {\n",
    "                name: torch.repeat(input[:, None], [1, self.hparams.num_samples]+[1]*(input.shape.ndims - 1))\n",
    "                for name, input in inputs.items()}\n",
    "            zs_samples_shape = [self.hparams.sequence_length - 1,\n",
    "                                self.hparams.num_samples,\n",
    "                                self.batch_size,\n",
    "                                self.hparams.nz]\n",
    "            if self.hparam.learn_prior:\n",
    "                eps = torch.randn(zs_samples_shape)\n",
    "                zs_prior_samples = (outputs_prior['zs_mu'][:, None] +\n",
    "                                torch.sqrt(torch.exp(outputs_prior['zs_log_sigma_sq']))[:, None] * eps)\n",
    "            else:\n",
    "                zs_prior_samples = torch.randn(\n",
    "                    [self.hparams.sequence_length - self.hparams.context_frames] + zs_samples_shape[1:])\n",
    "                zs_prior_samples = torch.cat(\n",
    "                    [torch.repeat(zs_posterior[:self.hparams.context_frames - 1][:, None],\n",
    "                                  [1, self.hparams.num_samples, 1, 1]),\n",
    "                     zs_prior_samples], dim=0)\n",
    "            inputs_prior_samples = dict(inputs_samples)\n",
    "            inputs_prior_samples['zs'] = zs_prior_samples\n",
    "            \n",
    "            ### 第1，2个维度压平 5/23\n",
    "            inputs_prior_samples = {name: torch.flatten(input, start_dim=1, end_dim=2)\n",
    "                                    for name, input in inputs_prior_samples.items()}\n",
    "            gen_outputs_samples = self.generator(inputs_prior_samples)\n",
    "            gen_images_samples = gen_outputs_samples['gen_images']\n",
    "            ### 再恢复出前两个维度 5/23\n",
    "            gen_images_samples = torch.stack(gen_images_samples.chunk(self.hparams.num_samples, dim=1), dim=-1)\n",
    "            gen_images_samples_avg = torch.mean(gen_images_samples, dim=-1)\n",
    "            outputs['gen_images_samples'] = gen_images_samples\n",
    "            outputs['gen_images_samples_avg'] = gen_images_samples_avg\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "images = torch.tensor(np.transpose(inputs,[1,0,4,2,3]))  # to DNCHW 5/22\n",
    "myG = Generator(images.shape, 'train', parsed_hparams)\n",
    "out = myG(images)\n",
    "for o in out.items():\n",
    "    print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 6, 256])\n",
      "z_mu torch.Size([28, 6, 8])\n",
      "z_log_sigma_sq torch.Size([28, 6, 8])\n"
     ]
    }
   ],
   "source": [
    "# 测试 prior\n",
    "# from module.py\n",
    "# liyi,2019/5/22\n",
    "\n",
    "import functools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from video_prediction.utils.max_sv import spectral_normed_weight\n",
    "from video_prediction.layers.conv import Conv2d, Conv3d\n",
    "from video_prediction.models.modules import Encoder,Dense\n",
    "\n",
    "class Prior(nn.Module):\n",
    "    ### 改写自savp_model.py prior_fn 5/16\n",
    "    ### inputs应当是 NDCHW 5/22\n",
    "    def __init__(self, input_shape, hparams):\n",
    "        super(Prior, self).__init__()\n",
    "        self.hparams = hparams\n",
    "        self.input_shape = list(input_shape)\n",
    "        ## inputs concat 过程中只取了 inputs[:self.hparams.context_frames - 1] 5/22\n",
    "        self.concat_shape = [(hparams.context_frames-1)%self.input_shape[0]]+ \\\n",
    "                            [self.input_shape[1]]+ \\\n",
    "                            [self.input_shape[-3]*2]+ \\\n",
    "                            self.input_shape[-2:]\n",
    "        encoder_input_shape = [np.prod(self.concat_shape[0:2])]+self.concat_shape[2:]\n",
    "        self.encoder = Encoder(input_shape=encoder_input_shape,\n",
    "                               nef=hparams.nef,\n",
    "                               n_layers=hparams.n_layers)### input_shape需要根据hparmas修改 5/16\n",
    "        \n",
    "        encoder_out_shape = self.concat_shape[0:2]+ \\\n",
    "                            [hparams.nef * min(4, 2**(hparams.n_layers-1))]\n",
    "        dense_input_shape = encoder_out_shape\n",
    "        dense_input_shape[0] += hparams.sequence_length - hparams.context_frames\n",
    "        dense_input_shape = [np.prod(dense_input_shape[0:2])] + dense_input_shape[2:]\n",
    "        self.dense0 = Dense(input_shape=dense_input_shape, units=hparams.nef * 4)\n",
    "        \n",
    "        ### lstm input 应为 (seq_len, batch, input_size) 5/22\n",
    "        rnn_input_shape = self.concat_shape[0:2]+[hparams.nef * 4]\n",
    "        if hparams.rnn == 'lstm':\n",
    "            # input_size: The number of expected features in the input x 5/23\n",
    "            # inputs: input, (h_0, c_0)\n",
    "            #         input of shape=(seq_len, batch, input_size) 5/23\n",
    "            # outputs: output, (h_n, c_n) 5/23\n",
    "            #          output of shape (seq_len, batch, num_directions * hidden_size) 5/23\n",
    "            self.rnn = nn.LSTM(input_size=hparams.nef * 4, hidden_size=hparams.nef * 4)\n",
    "        elif hparams.rnn == 'gru':\n",
    "            self.rnn = nn.GRU(input_size=hparams.nef * 4, hidden_size=hparams.nef * 4)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        rnn_output_shape = self.concat_shape[0:2]+[hparams.nef * 4]\n",
    "        dense_input_shape = [np.prod(rnn_output_shape[0:2])] + rnn_output_shape[2:]\n",
    "        self.dense1 = Dense(input_shape=dense_input_shape, units=hparams.nz)  ### input_shape要改 5/16\n",
    "        self.dense2 = Dense(input_shape=dense_input_shape, units=hparams.nz)  ### input_shape要改 5/16\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        ### inputs应当是 NDCHW 5/22\n",
    "        outputs = {}\n",
    "        ### 将连续的两帧图片在channel维度上级联 5/16\n",
    "        ### context_frams 需要根据 ... 5/16\n",
    "        inputs = torch.cat([inputs[:self.hparams.context_frames - 1],\n",
    "                            inputs[1:self.hparams.context_frames]], dim=-3)  \n",
    "        inputs = inputs.reshape([-1]+list(inputs.shape[-3:]))  ### 变为 NCHW 5/23\n",
    "        ### 加入 action uncompleted ... \n",
    "        h = self.encoder(inputs)['output']\n",
    "        h = h.reshape(self.concat_shape[0:2]+[-1])\n",
    "        \n",
    "        h_zeros = torch.zeros(size=[self.hparams.sequence_length-self.hparams.context_frames]+list(h.shape[1:]))\n",
    "        h = torch.cat([h, h_zeros], dim=0)\n",
    "        \n",
    "        h = h.reshape([-1, h.size(-1)])\n",
    "        h = self.dense0(h)\n",
    "        h = h.reshape(self.concat_shape[0:2]+[-1])\n",
    "        h = self.rnn(h)[0]\n",
    "        # print(len(h))      # 2 5/23\n",
    "        # print(h[0].shape)  # torch.Size([28, 6, 256]) 5/23\n",
    "        # print(len(h[1]))   # 2 5/23\n",
    "        # for k in h[1]:\n",
    "            # print(k.shape)\n",
    "            # torch.Size([1, 6, 256]) 5/23\n",
    "            # torch.Size([1, 6, 256])\n",
    "        \n",
    "        h = h.reshape([-1, h.size(-1)])\n",
    "        z_mu = self.dense1(h).reshape(self.concat_shape[0:2]+[-1])\n",
    "        outputs['z_mu'] = z_mu\n",
    "        z_log_sigma_sq = self.dense2(h).reshape(self.concat_shape[0:2]+[-1])\n",
    "        z_log_sigma_sq = torch.clamp(z_log_sigma_sq, -10,10)\n",
    "        outputs['z_log_sigma_sq'] = z_log_sigma_sq\n",
    "        return outputs\n",
    "        \n",
    "images = torch.tensor(np.transpose(inputs,[1,0,4,2,3]))  # to DNCHW 5/22\n",
    "net = Prior(input_shape=images.shape, hparams=parsed_hparams)\n",
    "output = net.forward(images)\n",
    "for k in output.keys():\n",
    "    # z_mu torch.Size([28, 6, 8])   5/23\n",
    "    # z_log_sigma_sq torch.Size([28, 6, 8])\n",
    "    print(k, output[k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[180, 256]\n",
      "torch.Size([174, 6, 160, 320])\n",
      "z_mu torch.Size([29, 6, 8])\n",
      "z_log_sigma_sq torch.Size([29, 6, 8])\n"
     ]
    }
   ],
   "source": [
    "# 测试 posterior\n",
    "# from modules.py\n",
    "# liyi,2019/5/22\n",
    "# input 为 DNCHW 5/22\n",
    "# output 为 D-1,N,nz[=8] 5/22\n",
    "\n",
    "import functools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from video_prediction.utils.max_sv import spectral_normed_weight\n",
    "from video_prediction.layers.conv import Conv2d, Conv3d\n",
    "from video_prediction.models.modules import Encoder,Dense\n",
    "\n",
    "class Posterior(nn.Module):\n",
    "    ### 改写自savp_model.py posterior_fn 5/15\n",
    "    ### input 为 DNCHW 5/19\n",
    "    ### output 为 D-1,N,nz[=8] 5/21\n",
    "    def __init__(self, input_shape, hparams):\n",
    "        super(Posterior, self).__init__()\n",
    "        self.input_shape = list(input_shape)\n",
    "        self.use_e_rnn = hparams.use_e_rnn  ### 默认false 5/19\n",
    "        \n",
    "        #input_shape = list(input_shape)\n",
    "        #input_shape=[np.prod(input_shape[0:2])]+[input_shape[-3]*2]+input_shape[-2:]\n",
    "        #print(input_shape)\n",
    "        self.encoder = Encoder(input_shape=[np.prod(self.input_shape[0:2])]+[self.input_shape[-3]*2]+self.input_shape[-2:],\n",
    "                               nef=hparams.nef, n_layers=hparams.n_layers)\n",
    "        out_shape = [np.prod(self.input_shape[0:2]),\n",
    "                     hparams.nef * min(4, 2**(hparams.n_layers-1))]\n",
    "        print(out_shape)\n",
    "        self.dense1 = Dense(input_shape=out_shape, units=hparams.nz)  ### input_shape要改 5/15\n",
    "        self.dense2 = Dense(input_shape=out_shape, units=hparams.nz)  ### input_shape要改 5/15\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        ### inputs应当是 NDCHW 5/16\n",
    "        outputs = {}\n",
    "        inputs = torch.cat([inputs[:-1], inputs[1:]], dim=-3) ### 将连续的两帧图片在channel维度上级联 5/16\n",
    "        inputs = inputs.reshape([-1]+list(inputs.shape[-3:]))  ### 变为 NCHW 5/22\n",
    "        print(inputs.shape)\n",
    "        ### 加入 action uncompleted ... \n",
    "        h = self.encoder(inputs)['output']\n",
    "        if self.use_e_rnn:\n",
    "            h = self.dense0(h)\n",
    "            h = self.rnn(h)\n",
    "        z_mu = self.dense1(h).reshape([self.input_shape[0]-1]+[self.input_shape[1]]+[-1])\n",
    "        outputs['z_mu'] = z_mu\n",
    "        z_log_sigma_sq = self.dense2(h).reshape([self.input_shape[0]-1]+[self.input_shape[1]]+[-1])\n",
    "        z_log_sigma_sq = torch.clamp(z_log_sigma_sq, -10,10)\n",
    "        outputs['z_log_sigma_sq'] = z_log_sigma_sq\n",
    "        return outputs\n",
    "    \n",
    "images = torch.tensor(np.transpose(inputs,[1,0,4,2,3]))  # to DNCHW 5/22\n",
    "net = Posterior(input_shape=images.shape, hparams=parsed_hparams)\n",
    "output = net.forward(images)\n",
    "for k in output.keys():\n",
    "    print(k, output[k].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 30, 320, 3, 160])\n",
      "conv:  torch.Size([30, 256, 1, 20])\n",
      "pool:  torch.Size([30, 256, 1, 1])\n",
      "torch.Size([30, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liyi/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "# 测试encoder\n",
    "# from modules.py\n",
    "# liyi，2019/5/22\n",
    "\n",
    "import functools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from video_prediction.utils.max_sv import spectral_normed_weight\n",
    "from video_prediction.layers.conv import Conv2d, Conv3d\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    ### conv2d的in_channels是否为3存疑 5/8\n",
    "    ### nef 为 encoder 的 filter 个数 5/9\n",
    "    ### conv2d 要求 input_shape = NCHW\n",
    "    ### input_shape = (ND/DN,CHW) 5/22\n",
    "    ### output_shape = (ND/DN,min(2**(n_layers-1),4)),只有三个维度 5/21\n",
    "    def __init__(self, input_shape, nef=64, n_layers=3):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.conv = {}\n",
    "        self.norm = {}\n",
    "        self.conv0 = nn.Conv2d(in_channels=self.input_shape[-3], \n",
    "                               out_channels=nef, kernel_size=4, stride=2, padding=(1,1))\n",
    "        def make_sequence(in_channel, i):\n",
    "            out_channel = nef * min(2**i, 4)\n",
    "            return [nn.Conv2d(\n",
    "                        in_channels=in_channel,\n",
    "                        out_channels=out_channel, \n",
    "                        kernel_size=4, stride=2,\n",
    "                        padding=(1,1)),\n",
    "                      nn.InstanceNorm2d(\n",
    "                        num_features=out_channel,\n",
    "                        eps=1e-6)], out_channel\n",
    "        \n",
    "        self.model_list = nn.ModuleList()\n",
    "        in_channel = nef\n",
    "        for i in range(1, n_layers):\n",
    "            sequence, in_channel = make_sequence(in_channel, i)\n",
    "            self.model_list += sequence\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        ### inputs 应当是 NCHW 5/8\n",
    "        outputs = {}\n",
    "        output = self.conv0(inputs)\n",
    "        output = F.leaky_relu(output, negative_slope=0.2)\n",
    "        n = 0\n",
    "        outputs['encoder_%d'%n] = output     ### for visualization 5/8\n",
    "        for model in self.model_list:\n",
    "            n += 1\n",
    "            output = model(output)\n",
    "            output = F.leaky_relu(output, negative_slope=0.2)\n",
    "            outputs['encoder_%d'%n] = output\n",
    "        print('conv: ',output.shape)\n",
    "        output = F.avg_pool2d(output, output.shape[2:])\n",
    "        print('pool: ',output.shape)\n",
    "        output.squeeze_(dim=-2)  # 对HW两个维度squeeze\n",
    "        output.squeeze_(dim=-1)\n",
    "        outputs['output'] = output\n",
    "        return outputs\n",
    "    \n",
    "inputs = torch.tensor(np.transpose(inputs,axes=[0,1,4,2,3])) # NDHWC to NDCHW 5/22\n",
    "print(inputs.shape)\n",
    "net = Encoder(input_shape=inputs[0].shape)\n",
    "outputs = net(inputs[0])\n",
    "print(outputs['output'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n",
      "[  2  12 160 320   3]\n",
      "[ -1 160 320   3]\n",
      "(4,)\n",
      "[ -1 160 320   3]\n",
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "ndims = 4\n",
    "x = tf.zeros([2,12,160,320,3])\n",
    "\n",
    "sess = tf.Session()\n",
    "print(x1.get_shape())\n",
    "\n",
    "shape = tf.shape(x)\n",
    "print(sess.run(shape))\n",
    "\n",
    "x1 = tf.concat([[-1], shape[-(ndims-1):]], axis=0)\n",
    "print(sess.run(x1))\n",
    "print(x1.get_shape())\n",
    "\n",
    "x1.set_shape([ndims])\n",
    "print(sess.run(x1))\n",
    "print(x1.get_shape())\n",
    "\n",
    "#sess = tf.Session()\n",
    "#print(sess.run(tf.shape(x1), feed_dict={x1:[0,1,2,3]}))\n",
    "#x1.set_shape([2,2])\n",
    "#print(sess.run(tf.shape(x1)))\n",
    "#print(sess.run(tf.shape(x1), feed_dict={x1:[[0,1],[2,3]]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 4],\n",
      "        [2, 3, 4]])\n",
      "tensor([[1, 2],\n",
      "        [4, 2],\n",
      "        [3, 4]])\n",
      "torch.Size([3, 2])\n",
      "tensor([[1, 2, 4, 2, 3, 4]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([[1,2,4],[2,3,4]])\n",
    "b = a.reshape([-1,2])\n",
    "print(a)\n",
    "print(b)\n",
    "print(b.shape)\n",
    "b = a.view([1,6])\n",
    "print(b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
